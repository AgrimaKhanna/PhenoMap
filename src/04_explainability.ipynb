{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LHSlGAyWfRCn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/agrima/miniconda3/envs/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import RGCNConv\n",
        "from torch_geometric.explain import Explainer, GNNExplainer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import tempfile\n",
        "import base64\n",
        "from IPython.display import IFrame\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "data_fp = '../data/PROCESSED/'\n",
        "model_fp = '../models'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "ZlHXou9XfRTD"
      },
      "outputs": [],
      "source": [
        "HF_TOKEN = input(\"Enter Hugging Face token: \").strip() # Enter hugging face token\n",
        "assert HF_TOKEN, \" Please provide valid Hugging Face token.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 17 files: 100%|██████████| 17/17 [00:00<00:00, 352985.98it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/home/agrima/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "snapshot_download(repo_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", repo_type=\"model\", token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XDB3qXIqjlCR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/agrima/miniconda3/envs/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.21it/s]\n"
          ]
        }
      ],
      "source": [
        "#  Load LLaMA 3.1 8B Instruct model\n",
        "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
        "llm = AutoModelForCausalLM.from_pretrained(model_name, token=HF_TOKEN, torch_dtype=torch.float16, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LWC19PrkbC3E"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_19763/719371915.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  hetero_dict = torch.load(f\"{model_fp}/hetero_data_dict_version_final.pt\", map_location=\"cpu\")\n"
          ]
        }
      ],
      "source": [
        "# Load trained model components\n",
        "hetero_dict = torch.load(f\"{model_fp}/hetero_data_dict_version_final.pt\", map_location=\"cpu\")\n",
        "hetero_data = HeteroData.from_dict(hetero_dict)\n",
        "with open(f\"{data_fp}/node_maps_version_final.pkl\", \"rb\") as f:\n",
        "    node_maps = pickle.load(f)\n",
        "\n",
        "relation_to_id = {rel: i for i, rel in enumerate(hetero_data.edge_types)}\n",
        "x = torch.cat([hetero_data[n].x for n in hetero_data.node_types], dim=0)\n",
        "edge_index_all, edge_type_all = [], []\n",
        "for etype, eidx in hetero_data.edge_index_dict.items():\n",
        "    rel_id = relation_to_id[etype]\n",
        "    edge_index_all.append(eidx)\n",
        "    edge_type_all.append(torch.full((eidx.size(1),), rel_id, dtype=torch.long))\n",
        "edge_index = torch.cat(edge_index_all, dim=1)\n",
        "edge_type = torch.cat(edge_type_all)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHkeXKX1Mgqh"
      },
      "source": [
        " ## LOADING MODEL AND PERFORMING EXPLAINABILITY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iYnSa8lDJ3V3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_19763/211032983.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(f\"{model_fp}/rgcn_distmult_multirel_metrics.pt\", map_location=\"cpu\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DistMultPredictor(\n",
              "  (encoder): RGCNEncoder(\n",
              "    (conv1): RGCNConv(128, 256, num_relations=11)\n",
              "    (conv2): RGCNConv(256, 128, num_relations=11)\n",
              "  )\n",
              "  (rel_embeddings): Embedding(11, 128)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Model definitions\n",
        "class RGCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_relations):\n",
        "        super().__init__()\n",
        "        self.conv1 = RGCNConv(in_channels, hidden_channels, num_relations)\n",
        "        self.conv2 = RGCNConv(hidden_channels, out_channels, num_relations)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_type):\n",
        "        x = self.conv1(x, edge_index, edge_type)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index, edge_type)\n",
        "        return x\n",
        "\n",
        "class DistMultPredictor(torch.nn.Module):\n",
        "    def __init__(self, encoder, embedding_dim, num_relations):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.rel_embeddings = torch.nn.Embedding(num_relations, embedding_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_type, edge_label_index, edge_type_ids):\n",
        "        z = self.encoder(x, edge_index, edge_type)\n",
        "        src = z[edge_label_index[0]]\n",
        "        dst = z[edge_label_index[1]]\n",
        "        rel = self.rel_embeddings(edge_type_ids)\n",
        "        return (src * rel * dst).sum(dim=-1)\n",
        "\n",
        "# Load trained model state\n",
        "embedding_dim = 128\n",
        "encoder = RGCNEncoder(128, 256, embedding_dim, len(relation_to_id))\n",
        "ckpt = torch.load(f\"{model_fp}/rgcn_distmult_multirel_metrics.pt\", map_location=\"cpu\")\n",
        "encoder.load_state_dict(ckpt['encoder_state_dict'])\n",
        "predictor = DistMultPredictor(encoder, embedding_dim, len(relation_to_id))\n",
        "predictor.rel_embeddings.load_state_dict({'weight': ckpt['decoder_state_dict']['relation_embeddings.weight']})\n",
        "predictor.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "362rcDlpEmN3"
      },
      "outputs": [],
      "source": [
        "def explain_query(disease, phenotype):\n",
        "    try:\n",
        "        src_idx = node_maps[\"disease\"].get(disease)\n",
        "        dst_idx = node_maps[\"phenotype\"].get(phenotype)\n",
        "        rel_tuple = (\"disease\", \"disease_phenotype_positive\", \"phenotype\")\n",
        "        rel_id = torch.tensor([relation_to_id[rel_tuple]])\n",
        "\n",
        "        print(\"FOUND DISEASE:\", disease, \"PHENOTYPE:\", phenotype)\n",
        "\n",
        "\n",
        "        if src_idx is None or dst_idx is None:\n",
        "            return \"Invalid disease or phenotype name.\", \"<p style='color:red;'> Disease or phenotype not found.</p>\"\n",
        "\n",
        "        edge_label_index = torch.tensor([[src_idx], [dst_idx]])\n",
        "\n",
        "        # Prediction score from full graph\n",
        "        with torch.no_grad():\n",
        "            full_score = predictor(x, edge_index, edge_type, edge_label_index, rel_id).item()\n",
        "            normalized_full_score = torch.sigmoid(torch.tensor(full_score)).item()\n",
        "\n",
        "        # GNNExplainer setup\n",
        "        explainer = Explainer(\n",
        "            model=predictor,\n",
        "            algorithm=GNNExplainer(epochs=75),\n",
        "            explanation_type=\"model\",\n",
        "            edge_mask_type=\"object\",\n",
        "            model_config=dict(\n",
        "                mode=\"binary_classification\",\n",
        "                task_level=\"edge\",\n",
        "                return_type=\"raw\",\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        print(\"Completed explainer setup.\")\n",
        "\n",
        "        explanation = explainer(\n",
        "            x=x,\n",
        "            edge_index=edge_index,\n",
        "            edge_type=edge_type,\n",
        "            edge_label_index=edge_label_index,\n",
        "            edge_type_ids=rel_id\n",
        "        )\n",
        "\n",
        "        edge_mask = explanation.edge_mask\n",
        "        top_edges = edge_mask.topk(15).indices\n",
        "        important_edges = edge_index[:, top_edges]\n",
        "\n",
        "        if important_edges.size(1) == 0:\n",
        "            return \"No important edges found for this prediction.\", \"<p>No influential subgraph detected.</p>\"\n",
        "\n",
        "        # Subgraph confidence\n",
        "        masked_edge_index = explanation.edge_index[:, explanation.edge_mask.bool()]\n",
        "        masked_edge_type = edge_type[explanation.edge_mask.bool()]\n",
        "        with torch.no_grad():\n",
        "            subgraph_score = predictor(x, masked_edge_index, masked_edge_type, edge_label_index, rel_id).item()\n",
        "            normalized_subgraph_score = torch.sigmoid(torch.tensor(subgraph_score)).item()\n",
        "\n",
        "        # Create graph\n",
        "        index_to_name = {v: k for t in node_maps for k, v in node_maps[t].items()}\n",
        "        G = nx.DiGraph()\n",
        "        for src, dst in important_edges.t().tolist():\n",
        "            G.add_edge(index_to_name.get(src, str(src)), index_to_name.get(dst, str(dst)))\n",
        "\n",
        "        print(\"Completed explainer setup.\")\n",
        "\n",
        "        net = Network(height=\"700px\", width=\"100%\", notebook=False, cdn_resources=\"in_line\", directed=True)\n",
        "        for node in G.nodes():\n",
        "            color = (\n",
        "                \"#A3C4F3\" if node == disease else\n",
        "                \"#FFB3C6\" if node == phenotype else\n",
        "                \"#D3F8E2\"\n",
        "            )\n",
        "            net.add_node(node, label=node, color=color, font={'size': 28, 'color': '#eeeeee'})\n",
        "\n",
        "        for src, dst in G.edges():\n",
        "            net.add_edge(src, dst)\n",
        "\n",
        "        net.set_options('''\n",
        "        {\n",
        "          \"nodes\": {\n",
        "            \"shape\": \"dot\",\n",
        "            \"size\": 25,\n",
        "            \"font\": { \"size\": 28, \"face\": \"arial\", \"color\": \"#eeeeee\" }\n",
        "          },\n",
        "          \"edges\": {\n",
        "            \"width\": 1.5,\n",
        "            \"color\": { \"color\": \"#cccccc\" },\n",
        "            \"smooth\": false\n",
        "          },\n",
        "          \"physics\": {\n",
        "            \"enabled\": true,\n",
        "            \"barnesHut\": {\n",
        "              \"gravitationalConstant\": -25000,\n",
        "              \"centralGravity\": 0.1,\n",
        "              \"springLength\": 300,\n",
        "              \"springConstant\": 0.04,\n",
        "              \"damping\": 0.15,\n",
        "              \"avoidOverlap\": 1\n",
        "            }\n",
        "          },\n",
        "          \"layout\": {\n",
        "            \"improvedLayout\": true\n",
        "          }\n",
        "        }\n",
        "        ''')\n",
        "\n",
        "        # Save and encode graph\n",
        "        with tempfile.NamedTemporaryFile(\"w+\", suffix=\".html\", delete=False) as tmp_file:\n",
        "            net.save_graph(tmp_file.name)\n",
        "            tmp_file.seek(0)\n",
        "            html_content = tmp_file.read()\n",
        "\n",
        "        dark_css = \"\"\"\n",
        "        <style>\n",
        "          body { background-color: #111111; margin: 0; padding: 0; }\n",
        "          #mynetwork { background-color: #111111 !important; }\n",
        "        </style>\n",
        "        \"\"\"\n",
        "        html_content = html_content.replace(\"</head>\", f\"{dark_css}</head>\")\n",
        "        encoded_html = base64.b64encode(html_content.encode(\"utf-8\")).decode(\"utf-8\")\n",
        "        iframe_html = f'''\n",
        "        <iframe src=\"data:text/html;base64,{encoded_html}\"\n",
        "                width=\"100%\" height=\"550px\" frameborder=\"0\"\n",
        "                sandbox=\"allow-scripts allow-same-origin\"></iframe>\n",
        "        '''\n",
        "\n",
        "        # LLM explanation\n",
        "        summary = \"\\n\".join([f\"- {index_to_name[src]} ⟶ {index_to_name[dst]}\" for src, dst in important_edges.t().tolist()])\n",
        "        prompt = f\"\"\"\n",
        "A biomedical GNN model predicted a relationship:\n",
        "- Disease: {disease}\n",
        "- Phenotype: {phenotype}\n",
        "\n",
        "Confidence Scores:\n",
        "- Full graph prediction (normalized): {normalized_full_score:.4f}\n",
        "- GNNExplainer subgraph prediction (normalized): {normalized_subgraph_score:.4f}\n",
        "\n",
        "Important edges influencing this prediction:\n",
        "{summary}\n",
        "\n",
        "Explain why this might make sense biologically, in a small paragraph. And don't write anything else.\n",
        "\"\"\"\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(llm.device)\n",
        "        outputs = llm.generate(**inputs, max_new_tokens=250)\n",
        "        explanation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        return explanation.strip(), iframe_html\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(\"Exception:\", traceback.format_exc())\n",
        "        return f\"Error: {str(e)}\", f\"<p style='color:red;'> {str(e)}</p>\" \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EYCarJyxEsfM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://35d40ad6319453daa4.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://35d40ad6319453daa4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FOUND DISEASE: psoriasis PHENOTYPE: abnormality of the skin\n",
            "Completed explainer setup.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed explainer setup.\n"
          ]
        }
      ],
      "source": [
        "with gr.Blocks(title=\"PhenoMap\") as demo:\n",
        "    gr.Markdown(\"Enter disease and phenotype to get explanation of how they're linked.\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            disease_input = gr.Textbox(label=\"Enter Disease\")\n",
        "            phenotype_input = gr.Textbox(label=\"Enter Phenotype\")\n",
        "            run_button = gr.Button(\"Run Explanation\")\n",
        "        with gr.Column(scale=2):\n",
        "            explanation_output = gr.Textbox(label=\"Prediction & Confidence\", lines=8, interactive=False)\n",
        "            graph_output = gr.HTML(label=\"Subgraph Visualization\")\n",
        "\n",
        "    run_button.click(fn=explain_query, inputs=[disease_input, phenotype_input], outputs=[explanation_output, graph_output])\n",
        "\n",
        "demo.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
